# ç»ˆæå®Œç¾ç‰ˆï¼šä¿®å¤åˆ†ç±»æŠ¥å‘Šç±»åˆ«åŒ¹é… + å…¨æµç¨‹æ— é”™
import os
import sys
import types

# ç¯å¢ƒå˜é‡é”å®š
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TF_USE_LEGACY_KERAS"] = "1"
os.environ["KERAS_BACKEND"] = "tensorflow"

# æ ¸å¿ƒä¾èµ–å¤„ç†ï¼šæ¨¡æ‹Ÿ keras.engine è§£å†³å¯¼å…¥é”™è¯¯
import tensorflow as tf
import tf_keras


# 1. æ¨¡æ‹Ÿ Keras 2.x æ‰€éœ€çš„ keras.engine.base_layer.Node
class MockNode:
    def __init__(self, *args, **kwargs):
        pass


# 2. æ„å»ºæ¨¡æ‹Ÿæ¨¡å—å±‚çº§
engine_module = types.ModuleType('keras.engine')
base_layer_module = types.ModuleType('keras.engine.base_layer')
base_layer_module.Node = MockNode
engine_module.base_layer = base_layer_module


# 3. åŒ…è£… tf_keras æ³¨å…¥ engine å±æ€§
class KerasWrapper(tf_keras.__class__):
    def __init__(self):
        self.__dict__.update(tf_keras.__dict__)
        self.engine = engine_module


# 4. æ›¿æ¢ç³»ç»Ÿæ¨¡å—æ˜ å°„
keras_wrapper = KerasWrapper()
sys.modules['keras'] = keras_wrapper
sys.modules['keras.backend'] = tf_keras.backend
sys.modules['keras.layers'] = tf_keras.layers
sys.modules['keras.models'] = tf_keras.models
sys.modules['keras.optimizers'] = tf_keras.optimizers
sys.modules['keras.losses'] = tf_keras.losses
sys.modules['keras.callbacks'] = tf_keras.callbacks
sys.modules['keras.engine'] = engine_module
sys.modules['keras.engine.base_layer'] = base_layer_module

# å…¶ä»–ä¾èµ–å¯¼å…¥
import re
import jieba
import requests
import numpy as np
import pandas as pd
import zipfile
from io import BytesIO
from sklearn.metrics import accuracy_score, classification_report
from bert4keras.backend import set_gelu
from bert4keras.models import build_transformer_model
from bert4keras.tokenizers import Tokenizer
from bert4keras.optimizers import Adam
from bert4keras.snippets import sequence_padding, DataGenerator

# åˆå§‹åŒ–é…ç½®
np.random.seed(42)
tf.random.set_seed(42)
set_gelu("tanh")
print("=== ç¯å¢ƒé…ç½®æ ¡éªŒ ===")
print(f"TensorFlow ç‰ˆæœ¬: {tf.__version__}")
print(f"tf_keras ç‰ˆæœ¬: {tf_keras.__version__}")
gpus = tf.config.list_physical_devices('GPU')
print(f"å¯ç”¨ GPU æ•°é‡: {len(gpus)}")
print("=" * 50)


# ===================== 1. å…¨å±€é…ç½® =====================
class Config:
    MODEL_CACHE_DIR = "./model_cache/"
    os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
    OFFICIAL_BERT_ZIP_URL = "https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip"
    ZIP_INNER_DIR = "chinese_L-12_H-768_A-12/"
    CONFIG_PATH = os.path.join(MODEL_CACHE_DIR, ZIP_INNER_DIR, "bert_config.json")
    CHECKPOINT_PATH = os.path.join(MODEL_CACHE_DIR, ZIP_INNER_DIR, "bert_model.ckpt")
    DICT_PATH = os.path.join(MODEL_CACHE_DIR, ZIP_INNER_DIR, "vocab.txt")
    TASK_TYPE = "multi_class"
    SAVE_DIR = "./bert_trained_model/"
    os.makedirs(SAVE_DIR, exist_ok=True)
    BATCH_SIZE = 1
    EPOCHS = 1
    LEARNING_RATE = 1e-5
    MAX_LEN = 32  # å›ºå®šåºåˆ—é•¿åº¦


# ===================== 2. BERT æ¨¡å‹ä¸‹è½½ä¸è§£å‹ =====================
def download_and_extract_bert(config):
    required_files = [
        config.CONFIG_PATH,
        config.CHECKPOINT_PATH + ".index",
        config.CHECKPOINT_PATH + ".data-00000-of-00001",
        config.DICT_PATH
    ]
    if all(os.path.exists(f) for f in required_files):
        print("âœ… å·²æ£€æµ‹åˆ° BERT æ¨¡å‹æ–‡ä»¶ï¼Œç›´æ¥å¤ç”¨")
        return config

    print("ğŸ“¥ å¼€å§‹ä¸‹è½½å®˜æ–¹ä¸­æ–‡ BERT æ¨¡å‹ï¼ˆçº¦ 400MBï¼‰...")
    try:
        response = requests.get(config.OFFICIAL_BERT_ZIP_URL, stream=True, timeout=120)
        response.raise_for_status()
        total_size = int(response.headers.get("content-length", 0))
        downloaded_size = 0

        zip_buffer = BytesIO()
        for chunk in response.iter_content(chunk_size=1024 * 1024):
            if chunk:
                zip_buffer.write(chunk)
                downloaded_size += len(chunk)
                progress = (downloaded_size / total_size) * 100 if total_size > 0 else 100
                print(f"ä¸‹è½½è¿›åº¦: {progress:.1f}%", end="\r")

        print("\nâœ… ä¸‹è½½å®Œæˆï¼Œå¼€å§‹è§£å‹æ¨¡å‹æ–‡ä»¶...")
        with zipfile.ZipFile(zip_buffer, 'r') as zip_ref:
            zip_ref.extractall(config.MODEL_CACHE_DIR)
        print(f"âœ… è§£å‹å®Œæˆï¼Œæ¨¡å‹æ–‡ä»¶ä¿å­˜è‡³: {config.MODEL_CACHE_DIR}")
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹ä¸‹è½½/è§£å‹å¤±è´¥: {str(e)}")
        print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åŒ…å¹¶è§£å‹è‡³ ./model_cache/")
        print(f"æ‰‹åŠ¨ä¸‹è½½é“¾æ¥: {config.OFFICIAL_BERT_ZIP_URL}")
        sys.exit(1)
    return config


# ===================== 3. è¯å…¸åŠ è½½ =====================
def load_bert_token_dict(dict_path):
    token_dict = {}
    with open(dict_path, encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            if '\t' in line:
                token, idx = line.split('\t', 1)
                token_dict[token] = int(idx)
            else:
                token_dict[line] = len(token_dict)
    return token_dict


# ===================== 4. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† =====================
def load_sample_data():
    print("âš ï¸  æœªæ£€æµ‹åˆ°è‡ªå®šä¹‰æ•°æ®é›†ï¼Œä½¿ç”¨ç¤ºä¾‹æƒ…æ„Ÿæ•°æ®")
    data = {
        "comment": ["ç”µå½±è¶…æ£’", "æ‰‹æœºå¤ªå·®", "å¤©æ°”å¾ˆå¥½", "é¤å…éš¾åƒ", "å‰§ç‰¹æ•ˆå¥½", "æœåŠ¡æå·®", "æ€§ä»·æ¯”ä¸€èˆ¬", "ä½“éªŒä¸é”™"],
        "multi_label": [0, 1, 2, 1, 0, 1, 2, 0]
    }
    train_df = pd.DataFrame(data).iloc[:5]
    val_df = pd.DataFrame(data).iloc[5:6]
    test_df = pd.DataFrame(data).iloc[6:]
    label_col = "multi_label"
    text_col = "comment"
    num_labels = len(train_df[label_col].unique())
    label_map = {0: "æ­£é¢", 1: "è´Ÿé¢", 2: "ä¸­æ€§"}
    print(f"ğŸ“Š ä»»åŠ¡ä¿¡æ¯ï¼š{Config.TASK_TYPE}ï¼Œç±»åˆ«æ•°ï¼š{num_labels}ï¼Œæ ‡ç­¾æ˜ å°„ï¼š{label_map}")
    return train_df, val_df, test_df, num_labels, label_col, text_col, label_map


def clean_text(text):
    if pd.isna(text):
        return ""
    text = re.sub(r"[^\u4e00-\u9fa5]", "", str(text))
    return text.strip()


# ===================== 5. æ•°æ®ç”Ÿæˆå™¨ =====================
class CustomDataGenerator(DataGenerator):
    def __init__(self, data, tokenizer, text_col, label_col, max_len, batch_size=32, shuffle=True):
        self.tokenizer = tokenizer
        self.text_col = text_col
        self.label_col = label_col
        self.max_len = max_len
        super().__init__(data, batch_size, shuffle)
        self.data_len = len(data)

    def __iter__(self, random=False):
        batch_token_ids, batch_segment_ids, batch_labels = [], [], []
        for is_end, item in self.sample(random):
            text = item[self.text_col]
            if not text:
                token_ids = [0] * self.max_len
                segment_ids = [0] * self.max_len
            else:
                token_ids, segment_ids = self.tokenizer.encode(text, maxlen=self.max_len)
                if len(token_ids) < self.max_len:
                    token_ids += [0] * (self.max_len - len(token_ids))
                    segment_ids += [0] * (self.max_len - len(segment_ids))
                elif len(token_ids) > self.max_len:
                    token_ids = token_ids[:self.max_len]
                    segment_ids = segment_ids[:self.max_len]
            batch_token_ids.append(token_ids)
            batch_segment_ids.append(segment_ids)
            batch_labels.append([item[self.label_col]])

            if len(batch_token_ids) == self.batch_size or is_end:
                padded_token_ids = sequence_padding(batch_token_ids, length=self.max_len)
                padded_segment_ids = sequence_padding(batch_segment_ids, length=self.max_len)
                yield (
                    np.array(padded_token_ids, dtype=np.int32),
                    np.array(padded_segment_ids, dtype=np.int32),
                    np.array(batch_labels, dtype=np.int32)
                )
                batch_token_ids, batch_segment_ids, batch_labels = [], [], []

    def to_tf_dataset(self):
        def generator_fn():
            for token_ids, segment_ids, labels in self:
                yield (
                    {
                        "Input-Token": token_ids,  # å½¢çŠ¶ï¼š(1, 32)
                        "Input-Segment": segment_ids  # å½¢çŠ¶ï¼š(1, 32)
                    },
                    labels  # å½¢çŠ¶ï¼š(1, 1)
                )

        dataset = tf.data.Dataset.from_generator(
            generator_fn,
            output_signature=(
                {
                    "Input-Token": tf.TensorSpec(shape=(self.batch_size, self.max_len), dtype=tf.int32),
                    "Input-Segment": tf.TensorSpec(shape=(self.batch_size, self.max_len), dtype=tf.int32)
                },
                tf.TensorSpec(shape=(self.batch_size, 1), dtype=tf.int32)
            )
        )
        return dataset.prefetch(tf.data.AUTOTUNE)


# ===================== 6. æ¨¡å‹æ„å»º =====================
def build_bert_classifier(config, num_labels):
    try:
        bert_base = build_transformer_model(
            config_path=config.CONFIG_PATH,
            checkpoint_path=config.CHECKPOINT_PATH,
            model="bert",
            return_keras_model=True,
            verbose=0
        )
    except Exception as e:
        print(f"âŒ BERT åŸºç¡€æ¨¡å‹æ„å»ºå¤±è´¥: {str(e)}")
        sys.exit(1)

    # æå– CLS token ç‰¹å¾ï¼ˆå¥å­çº§ç‰¹å¾ï¼‰
    cls_output = tf_keras.layers.Lambda(lambda x: x[:, 0, :])(bert_base.output)

    output = tf_keras.layers.Dropout(rate=0.1, seed=42)(cls_output)
    output = tf_keras.layers.Dense(
        units=num_labels,
        activation="softmax",
        kernel_initializer=tf_keras.initializers.TruncatedNormal(stddev=0.02, seed=42)
    )(output)

    model = tf_keras.models.Model(inputs=bert_base.input, outputs=output)
    model.compile(
        optimizer=Adam(learning_rate=config.LEARNING_RATE),
        loss=tf_keras.losses.SparseCategoricalCrossentropy(from_logits=False),
        metrics=["accuracy"]
    )
    return model


# ===================== 7. æ¨¡å‹è¯„ä¼°ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šç±»åˆ«åŒ¹é…ï¼‰=====================
def evaluate_model_performance(model, test_generator, label_map):
    print("\n=== æ¨¡å‹è¯„ä¼°å¼€å§‹ ===")
    y_true = []
    y_pred = []
    for token_ids, segment_ids, labels in test_generator:
        pred = model.predict(
            {
                "Input-Token": token_ids,
                "Input-Segment": segment_ids
            },
            verbose=0
        )
        y_pred.extend(pred.argmax(axis=1))
        y_true.extend(labels.flatten())

    if len(y_true) == 0:
        print("âš ï¸  æµ‹è¯•é›†æ— æœ‰æ•ˆæ ·æœ¬ï¼Œè·³è¿‡è¯„ä¼°")
        return {"accuracy": 0.0}

    accuracy = accuracy_score(y_true, y_pred)
    print(f"å‡†ç¡®ç‡: {accuracy:.4f}")

    # æ ¸å¿ƒä¿®å¤ï¼šç­›é€‰æµ‹è¯•é›†å®é™…å­˜åœ¨çš„ç±»åˆ«ï¼Œç¡®ä¿ target_names æ•°é‡åŒ¹é…
    actual_labels = sorted(list(set(y_true)))  # å®é™…å­˜åœ¨çš„ç±»åˆ«ï¼ˆå»é‡+æ’åºï¼‰
    actual_target_names = [label_map[label] for label in actual_labels]  # å¯¹åº”æ ‡ç­¾åç§°

    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(
        y_true, y_pred,
        labels=actual_labels,  # æŒ‡å®šå®é™…å­˜åœ¨çš„ç±»åˆ«
        target_names=actual_target_names,  # åŒ¹é…å®é™…ç±»åˆ«çš„æ ‡ç­¾åç§°
        zero_division=0
    ))
    print("=" * 50)
    return {"accuracy": accuracy}


# ===================== 8. é¢„æµ‹å‡½æ•° =====================
def predict_single_text(text, model, tokenizer, config, label_map, threshold=0.5):
    cleaned_text = clean_text(text)
    if not cleaned_text:
        return "æ— æ•ˆæ–‡æœ¬ï¼ˆæ— ä¸­æ–‡å†…å®¹ï¼‰", 0.0
    token_ids, segment_ids = tokenizer.encode(cleaned_text, maxlen=config.MAX_LEN)
    if len(token_ids) < config.MAX_LEN:
        token_ids += [0] * (config.MAX_LEN - len(token_ids))
        segment_ids += [0] * (config.MAX_LEN - len(segment_ids))
    elif len(token_ids) > config.MAX_LEN:
        token_ids = token_ids[:config.MAX_LEN]
        segment_ids = segment_ids[:config.MAX_LEN]
    pred = model.predict(
        {
            "Input-Token": np.array([token_ids]),
            "Input-Segment": np.array([segment_ids])
        },
        verbose=0
    )[0]
    pred_label_id = np.argmax(pred)
    confidence = pred[pred_label_id]
    return (label_map[pred_label_id], confidence) if confidence >= threshold else ("ä¸ç¡®å®šï¼ˆç½®ä¿¡åº¦ä¸è¶³ï¼‰", confidence)


# ===================== 9. ä¸»å‡½æ•°ï¼ˆå…¨æµç¨‹æ— é”™ï¼‰=====================
def main():
    config = Config()
    config = download_and_extract_bert(config)

    # åŠ è½½æ•°æ®
    train_df, val_df, test_df, num_labels, label_col, text_col, label_map = load_sample_data()

    # åŠ è½½è¯å…¸
    print(f"\nğŸ”¤ åŠ è½½è¯å…¸æ–‡ä»¶: {config.DICT_PATH}")
    try:
        token_dict = load_bert_token_dict(config.DICT_PATH)
        tokenizer = Tokenizer(token_dict=token_dict, do_lower_case=True)
        print("âœ… è¯å…¸åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è¯å…¸åŠ è½½å¤±è´¥: {str(e)}")
        sys.exit(1)

    # æ–‡æœ¬æ¸…æ´—
    train_df["cleaned_text"] = train_df[text_col].apply(clean_text)
    val_df["cleaned_text"] = val_df[text_col].apply(clean_text)
    test_df["cleaned_text"] = test_df[text_col].apply(clean_text)
    text_col = "cleaned_text"

    # åˆ›å»ºç”Ÿæˆå™¨å¹¶è½¬æ¢ä¸º Dataset
    train_generator = CustomDataGenerator(
        data=train_df.to_dict("records"),
        tokenizer=tokenizer,
        text_col=text_col,
        label_col=label_col,
        max_len=config.MAX_LEN,
        batch_size=config.BATCH_SIZE,
        shuffle=True
    )
    train_dataset = train_generator.to_tf_dataset()

    val_generator = CustomDataGenerator(
        data=val_df.to_dict("records"),
        tokenizer=tokenizer,
        text_col=text_col,
        label_col=label_col,
        max_len=config.MAX_LEN,
        batch_size=config.BATCH_SIZE,
        shuffle=False
    )
    val_dataset = val_generator.to_tf_dataset()

    test_generator = CustomDataGenerator(
        data=test_df.to_dict("records"),
        tokenizer=tokenizer,
        text_col=text_col,
        label_col=label_col,
        max_len=config.MAX_LEN,
        batch_size=config.BATCH_SIZE,
        shuffle=False
    )

    # æ„å»ºæ¨¡å‹
    model = build_bert_classifier(config, num_labels)
    print(f"\nâœ… å®Œæ•´æ¨¡å‹æ„å»ºæˆåŠŸï¼Œæ€»å‚æ•°: {model.count_params():,}")
    print("\nğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒï¼ˆCPU ç¯å¢ƒï¼Œ1 è½®è®­ç»ƒï¼‰...")

    # è®­ç»ƒæ¨¡å‹
    model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=config.EPOCHS,
        steps_per_epoch=len(train_generator),
        validation_steps=len(val_generator),
        callbacks=[
            tf_keras.callbacks.EarlyStopping(
                monitor="val_accuracy",
                patience=0,
                mode="max",
                restore_best_weights=True
            )
        ],
        verbose=1
    )

    # è¯„ä¼°ä¸ä¿å­˜
    evaluate_model_performance(model, test_generator, label_map)
    weight_save_path = os.path.join(config.SAVE_DIR, "bert_classifier_best.weights")
    model.save_weights(weight_save_path)
    print(f"âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜è‡³: {weight_save_path}")

    # é¢„æµ‹ç¤ºä¾‹
    print("\n=== é¢„æµ‹ç¤ºä¾‹ ===")
    test_texts = ["äº§å“è´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨ä¸å€¼è¿™ä¸ªä»·", "ä½“éªŒå¾ˆå¥½ï¼Œè¶…å‡ºé¢„æœŸ", "ä¸€èˆ¬èˆ¬ï¼Œæ²¡ä»€ä¹ˆç‰¹åˆ«çš„", "åƒåœ¾äº§å“ï¼Œåƒä¸‡åˆ«ä¹°"]
    for text in test_texts:
        pred_label, pred_confidence = predict_single_text(text, model, tokenizer, config, label_map)
        print(f"æ–‡æœ¬: {text}")
        print(f"é¢„æµ‹ç»“æœ: {pred_label}ï¼Œç½®ä¿¡åº¦: {pred_confidence:.3f}")
        print("-" * 30)


if __name__ == "__main__":
    main()