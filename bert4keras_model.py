import pandas as pd
import jieba
import re
import numpy as np
from bert4keras.models import build_transformer_model
from bert4keras.tokenizers import Tokenizer
from sklearn.linear_model import LogisticRegression
from sklearn.multioutput import MultiOutputClassifier
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer

# é…ç½®BERTæ¨¡å‹ï¼ˆè‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒçš„ä¸­æ–‡BERTï¼‰
config_path = 'https://cdn.keras.io/models/bert-base-chinese/config.json'
checkpoint_path = 'https://cdn.keras.io/models/bert-base-chinese/bert_model.ckpt'
dict_path = 'https://cdn.keras.io/models/bert-base-chinese/vocab.txt'

print("æ­£åœ¨åŠ è½½BERTæ¨¡å‹...")
# åˆå§‹åŒ–tokenizerå’Œæ¨¡å‹
tokenizer = Tokenizer(dict_path, do_lower_case=False)
model = build_transformer_model(config_path, checkpoint_path)
print("âœ… BERTæ¨¡å‹åŠ è½½æˆåŠŸï¼")

# æ•°æ®å‡†å¤‡
test_data = pd.DataFrame({
    "comment": [
        "è¿™éƒ¨ç”µå½±å‰§æƒ…è¶…æ£’ï¼Œæ¼”æŠ€åœ¨çº¿ï¼Œæ¨èå¤§å®¶çœ‹ï¼",
        "æ–°ä¹°çš„æ‰‹æœºç»­èˆªå¤ªå·®ï¼Œå”®åè¿˜æ•·è¡ï¼Œå¤ªå‘äº†ï¼",
        "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ï¼Œå¿ƒæƒ…å¾ˆå¥½ï½",
        "è¿™ä¸ªé¤å…çš„èœåˆè´µåˆéš¾åƒï¼Œé¿é›·ï¼"
    ],
    "scene_label": ["å½±è§†è¯„ä»·", "äº§å“åæ§½", "æ—¥å¸¸åˆ†äº«", "æ¶ˆè´¹ä½“éªŒ"],
    "emotion_label": ["æ­£é¢", "è´Ÿé¢", "æ­£é¢", "è´Ÿé¢"],
    "multi_labels": ["å½±è§†,æ¨è", "äº§å“,å·®è¯„", "æ—¥å¸¸,å¼€å¿ƒ", "æ¶ˆè´¹,é¿é›·"]
})

# é¢„å¤„ç†å‡½æ•°
stop_words = {"çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "ä»¬", "åœ¨", "æœ‰", "å°±", "éƒ½"}


def clean_text(text):
    text = re.sub(r"[^\u4e00-\u9fa5]", "", text)
    words = jieba.lcut(text)
    words = [w for w in words if w not in stop_words and len(w) > 1]
    return " ".join(words)


test_data["cleaned_comment"] = test_data["comment"].apply(clean_text)


# BERTç‰¹å¾æå–å‡½æ•°
def get_bert_features(texts):
    features = []
    for text in texts:
        token_ids, segment_ids = tokenizer.encode(text, maxlen=128)
        # è·å–BERTè¾“å‡ºï¼ˆCLS tokençš„å‘é‡ï¼‰
        vec = model.predict([np.array([token_ids]), np.array([segment_ids])])[0]
        features.append(vec[0])  # å–ç¬¬ä¸€ä¸ªtokenï¼ˆCLSï¼‰çš„ç‰¹å¾
    return np.array(features)


print("æ­£åœ¨æå–BERTç‰¹å¾...")
# æå–æ‰€æœ‰æ–‡æœ¬çš„BERTç‰¹å¾
bert_features = get_bert_features(test_data["cleaned_comment"].tolist())
print(f"âœ… BERTç‰¹å¾æå–å®Œæˆï¼Œç‰¹å¾å½¢çŠ¶: {bert_features.shape}")

# æ ‡ç­¾ç¼–ç å’Œæ¨¡å‹è®­ç»ƒ
print("æ­£åœ¨è®­ç»ƒåˆ†ç±»æ¨¡å‹...")
scene_encoder = LabelEncoder().fit(test_data["scene_label"])
emotion_encoder = LabelEncoder().fit(test_data["emotion_label"])
mlb = MultiLabelBinarizer().fit(test_data["multi_labels"].str.split(","))

# è®­ç»ƒåˆ†ç±»å™¨
scene_clf = LogisticRegression(max_iter=1000).fit(bert_features, scene_encoder.transform(test_data["scene_label"]))
emotion_clf = LogisticRegression(max_iter=1000).fit(bert_features,
                                                    emotion_encoder.transform(test_data["emotion_label"]))
multi_clf = MultiOutputClassifier(LogisticRegression(max_iter=1000)).fit(bert_features, mlb.transform(
    test_data["multi_labels"].str.split(",")))

print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")


# é¢„æµ‹å‡½æ•°
def predict(comment):
    cleaned = clean_text(comment)
    feat = get_bert_features([cleaned])
    scene_pred = scene_encoder.inverse_transform(scene_clf.predict(feat))[0]
    emotion_pred = emotion_encoder.inverse_transform(emotion_clf.predict(feat))[0]
    multi_pred = mlb.inverse_transform(multi_clf.predict(feat))[0]

    return {
        "è¾“å…¥æ–‡æœ¬": comment,
        "åœºæ™¯åˆ†ç±»": scene_pred,
        "æƒ…æ„Ÿå€¾å‘": emotion_pred,
        "å¤šæ ‡ç­¾": multi_pred
    }


# æµ‹è¯•é¢„æµ‹
print("\n=== æµ‹è¯•é¢„æµ‹ç»“æœ ===")
test_comments = [
    "è¿™éƒ¨ç§‘å¹»ç”µå½±ç‰¹æ•ˆå¤ªæ£’äº†ï¼Œå‰§æƒ…ä¹Ÿå¾ˆç²¾å½©ï¼",
    "è¿™ä¸ªå“ç‰Œçš„æ‰‹æœºè´¨é‡å¤ªå·®ï¼Œç”¨äº†ä¸€å‘¨å°±åäº†ï¼"
]

for comment in test_comments:
    result = predict(comment)
    print(f"\n{result}")

print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")